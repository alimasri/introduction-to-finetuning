# Introduction to Fine-tuning

## Overview

This repository provides an introduction to **Fine-tuning** and focuses on **Parameter Efficient Fine-tunint** with **Low-rank Adaptation (LoRA)**. LoRA significantly reduces the computational and storage requirements of traditional fine-tuning methods by decomposing weight updates into low-rank matrices.

## What is covered?

- Explanation of fine-tuning challenges and solutions
- Detailed breakdown of LoRA and its benefits
- Fine-tuning Qwen1.5b for SPII detection use-case using LLamaFactory

Colab notebook: [Fine-tuning Qwen1.5b with LoRA](https://drive.google.com/file/d/1hVrY3edfO_-9bu0TKp93N_pUTpx7JbPp/view)

## References

- LoRA Paper: <https://arxiv.org/abs/2106.09685>
- Fine-tuning guide: <https://www.lakera.ai/blog/llm-fine-tuning-guide>
- Backpropagation: <https://hmkcode.com/ai/backpropagation-step-by-step/>

## Author

**Ali Masri** | Machine Learning Engineer

<https://www.alimasri.info>

Follow me on [LinkedIn](https://www.linkedin.com/in/alimasri/) | [GitHub](https://github.com/alimasri/)
